{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API download (Use Final product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "# Define the download directory\n",
    "download_path = r\"C:\\Users\\userAdmin\\Desktop\\Pipeline\\Data files\\IMERG temporary\\2022\"\n",
    "os.makedirs(download_path, exist_ok=True)\n",
    "\n",
    "# Read the URLs from the text file\n",
    "# Remove first 2 rows of the file!!!!\n",
    "urls_file = r\"c:\\Users\\userAdmin\\Downloads\\subset_GPM_3IMERGHH_07_20240621_062121_.txt\"\n",
    "try:\n",
    "    df = pd.read_csv(urls_file, header=None, delimiter=\"\\t\", on_bad_lines='warn')\n",
    "    urls = df[0].tolist()\n",
    "except Exception as e:\n",
    "    print(f\"Error reading URLs file: {e}\")\n",
    "    urls = []\n",
    "\n",
    "# Function to get filenames from URLs\n",
    "def get_filename_from_url(url):\n",
    "    date_time_index = url.find(\"IMERG.\")\n",
    "    if date_time_index != -1:\n",
    "        date_time_part = url[date_time_index+6:date_time_index+15] + url[date_time_index+16:date_time_index+20]\n",
    "        return date_time_part + \".nc\"\n",
    "    else:\n",
    "        return \"unknown_date_time.nc\"\n",
    "\n",
    "# Create a mapping of URLs to filenames\n",
    "url_to_filename = {url: get_filename_from_url(url) for url in urls}\n",
    "\n",
    "# Function to download a file\n",
    "def download_file(session, url, filename):\n",
    "    filepath = os.path.join(download_path, filename)\n",
    "    attempts = 0\n",
    "    max_attempts = 5\n",
    "    backoff_time = 1  # start with 1 second\n",
    "\n",
    "    while attempts < max_attempts:\n",
    "        try:\n",
    "            res = session.get(url)\n",
    "            res.raise_for_status()\n",
    "            with open(filepath, 'wb') as f:\n",
    "                f.write(res.content)\n",
    "            return f\"{filename} downloaded successfully.\"\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            attempts += 1\n",
    "            if attempts < max_attempts:\n",
    "                time.sleep(backoff_time)\n",
    "                backoff_time *= 2  # exponential backoff\n",
    "            else:\n",
    "                return f\"Failed to download {filename} after {attempts} attempts: {e}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error saving {filename}: {e}\"\n",
    "\n",
    "# Set up the session with retries\n",
    "session = requests.Session()\n",
    "retry = Retry(total=5, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])\n",
    "adapter = HTTPAdapter(max_retries=retry)\n",
    "session.mount('http://', adapter)\n",
    "session.mount('https://', adapter)\n",
    "\n",
    "# Download the files initially\n",
    "def download_files(url_to_filename):\n",
    "    with ThreadPoolExecutor(max_workers=100) as executor:\n",
    "        future_to_url = {executor.submit(download_file, session, url, filename): url for url, filename in url_to_filename.items()}\n",
    "        for future in as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                print(result)\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading {url}: {e}\")\n",
    "\n",
    "download_files(url_to_filename)\n",
    "\n",
    "# Check for missing files and redownload if necessary\n",
    "def check_and_redownload(url_to_filename):\n",
    "    # List the actual downloaded files\n",
    "    actual_files = os.listdir(download_path)\n",
    "    \n",
    "    # Identify missing files\n",
    "    missing_files = [filename for filename in url_to_filename.values() if filename not in actual_files]\n",
    "    \n",
    "    # If there are missing files, reconstruct their URLs and redownload them\n",
    "    if missing_files:\n",
    "        print(\"Retrying missing files...\")\n",
    "        missing_url_to_filename = {url: filename for url, filename in url_to_filename.items() if filename in missing_files}\n",
    "        download_files(missing_url_to_filename)\n",
    "    \n",
    "    # Final check to ensure all files are downloaded\n",
    "    actual_files = os.listdir(download_path)\n",
    "    still_missing = [filename for filename in url_to_filename.values() if filename not in actual_files]\n",
    "    \n",
    "    if still_missing:\n",
    "        print(\"Some files are still missing:\")\n",
    "        for filename in still_missing:\n",
    "            print(filename)\n",
    "    else:\n",
    "        print(\"All files downloaded successfully.\")\n",
    "\n",
    "check_and_redownload(url_to_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge within specific time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "def find_files_in_range(folder_path, start_date, end_date):\n",
    "    all_files = os.listdir(folder_path)\n",
    "    files_in_range = []\n",
    "\n",
    "    for file in all_files:\n",
    "        if file.endswith(\".nc\"):\n",
    "            file_date_str = file.split(\"-\")[0]  # Extract the date-time part\n",
    "            try:\n",
    "                file_date = pd.to_datetime(file_date_str, format='%Y%m%d')\n",
    "                if start_date <= file_date <= end_date:\n",
    "                    files_in_range.append(os.path.join(folder_path, file))\n",
    "            except ValueError:\n",
    "                print(f\"Skipping file with invalid date format: {file}\")\n",
    "    print(f\"Found {len(files_in_range)} files in the specified date range.\")\n",
    "    return sorted(files_in_range)\n",
    "\n",
    "def combine_nc_files(files):\n",
    "    datasets = []\n",
    "    for file in files:\n",
    "        try:\n",
    "            print(f\"Attempting to open file: {file}\")\n",
    "            ds = xr.open_dataset(file, engine='netcdf4')\n",
    "            datasets.append(ds)\n",
    "        except Exception as e:\n",
    "            print(f\"Error opening {file}: {e}\")\n",
    "    if not datasets:\n",
    "        raise ValueError(\"No valid datasets to combine.\")\n",
    "    combined_dataset = xr.concat(datasets, dim='time')\n",
    "    return combined_dataset\n",
    "\n",
    "def main():\n",
    "    folder_path = r'c:\\Users\\userAdmin\\Desktop\\Rain data files\\IMERG 10yrs\\data files'\n",
    "    start_date = pd.to_datetime('2022-01-01 00:00')\n",
    "    end_date = pd.to_datetime('2022-12-31 23:59')\n",
    "\n",
    "    files_in_range = find_files_in_range(folder_path, start_date, end_date)\n",
    "    if not files_in_range:\n",
    "        print(\"No files found in the specified date range.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        combined_dataset = combine_nc_files(files_in_range)\n",
    "        output_file = r'C:\\Users\\userAdmin\\Desktop\\Pipeline\\Data files\\IMERG_2022.nc'\n",
    "        combined_dataset.to_netcdf(output_file)\n",
    "        print(f\"Combined dataset saved to {output_file}\")\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

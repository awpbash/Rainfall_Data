{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GSMaP Grid\n",
    "\n",
    "This notebook serves as a platform to mass download gridded GSMaP data via the File Transfer Protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import dask\n",
    "import tqdm\n",
    "import ftplib\n",
    "import os\n",
    "import gzip\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import calendar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and extract\n",
    "\n",
    "This code block will automatically download all files within a date range and execute the following processes:\n",
    "1) Convert binary zip file to nc file\n",
    "2) Extract out relevant regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_binary_to_nc(binary_file, nc_file, nx=3600, ny=1200, dx=0.1, dy=0.1):\n",
    "    # Define the coordinates\n",
    "    lon = np.linspace(0 + dx / 2, 360 - dx / 2, nx)\n",
    "    lat = np.linspace(60 - dy / 2, -60 + dy / 2, ny)\n",
    "\n",
    "    # Read the binary data\n",
    "    data = np.fromfile(binary_file, dtype=np.float32)\n",
    "    \n",
    "    # Check if the data size matches the expected size\n",
    "    if data.size != nx * ny:\n",
    "        print(f\"Error: Data size {data.size} does not match expected size {nx * ny} for file {binary_file}\")\n",
    "        return False\n",
    "\n",
    "    data = data.reshape(ny, nx)\n",
    "    \n",
    "    # Create an xarray Dataset\n",
    "    ds = xr.Dataset(\n",
    "        {\n",
    "            'precipitation': (('lat', 'lon'), data)\n",
    "        },\n",
    "        coords={\n",
    "            'lon': lon,\n",
    "            'lat': lat\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Set attributes\n",
    "    ds['precipitation'].attrs['units'] = 'mm/hr'\n",
    "    ds['precipitation'].attrs['long_name'] = 'GSMaP Precipitation'\n",
    "    \n",
    "    # Save to NetCDF\n",
    "    ds.to_netcdf(nc_file)\n",
    "    return True\n",
    "\n",
    "def extract_useful_coordinates(nc_file, subset_nc_file, area_of_interest):\n",
    "    # Open the NetCDF file\n",
    "    ds = xr.open_dataset(nc_file)\n",
    "    \n",
    "    # Print the latitude and longitude bounds of the dataset\n",
    "    #print(f\"Dataset latitude bounds: {ds['lat'].min().values}, {ds['lat'].max().values}\")\n",
    "    #print(f\"Dataset longitude bounds: {ds['lon'].min().values}, {ds['lon'].max().values}\")\n",
    "\n",
    "    # Subset the data based on the area of interest\n",
    "    subset_ds = ds.sel(\n",
    "        lon=slice(area_of_interest['lon_min'], area_of_interest['lon_max']),\n",
    "        lat=slice(area_of_interest['lat_max'], area_of_interest['lat_min'])\n",
    "    )\n",
    "\n",
    "    # Check if the subsetted data is empty\n",
    "    if subset_ds['precipitation'].size == 0 or subset_ds['precipitation'].isnull().all():\n",
    "        print(\"No numeric data to plot.\")\n",
    "        return False\n",
    "    \n",
    "    # Save the subsetted data to a new NetCDF file\n",
    "    subset_ds.to_netcdf(subset_nc_file)\n",
    "    return True\n",
    "\n",
    "def download_and_process_file(ftp, file, local_dir, extracted_dir, area_of_interest, year, month):\n",
    "    local_filepath = os.path.join(local_dir, file)\n",
    "    subset_nc_file_path = os.path.join(extracted_dir, str(year), month, file.replace('.gz', '.nc'))\n",
    "\n",
    "    try:\n",
    "        with open(local_filepath, 'wb') as local_file:\n",
    "            ftp.retrbinary(f'RETR {file}', local_file.write)\n",
    "\n",
    "\n",
    "        # Unzip the file\n",
    "        with gzip.open(local_filepath, 'rb') as f_in:\n",
    "            with open(local_filepath.replace('.gz', ''), 'wb') as f_out:\n",
    "                f_out.write(f_in.read())\n",
    "\n",
    "        # Convert binary to NetCDF\n",
    "        if not convert_binary_to_nc(local_filepath.replace('.gz', ''), local_filepath.replace('.gz', '.nc')):\n",
    "            return\n",
    "        \n",
    "        # Ensure year/month subdirectories exist\n",
    "        os.makedirs(os.path.dirname(subset_nc_file_path), exist_ok=True)\n",
    "        \n",
    "        # Extract useful coordinates\n",
    "        extract_useful_coordinates(local_filepath.replace('.gz', '.nc'), subset_nc_file_path, area_of_interest)\n",
    "        \n",
    "        # Clean up\n",
    "        os.remove(local_filepath)\n",
    "        os.remove(local_filepath.replace('.gz', ''))\n",
    "        os.remove(local_filepath.replace('.gz', '.nc'))\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")\n",
    "def download_gz_files(ftp_host, ftp_user, ftp_password, remote_dir, local_dir, extracted_dir, start_year, end_year, area_of_interest):\n",
    "    try:\n",
    "        # Connect to FTP server\n",
    "        ftp = ftplib.FTP(ftp_host)\n",
    "        ftp.login(user=ftp_user, passwd=ftp_password)\n",
    "\n",
    "        # Ensure local and extracted directories exist\n",
    "        os.makedirs(local_dir, exist_ok=True)\n",
    "        os.makedirs(extracted_dir, exist_ok=True)\n",
    "\n",
    "        # Iterate through the years in the specified range\n",
    "        for year in range(start_year, end_year + 1):\n",
    "            start_m = start_month if year == start_year else 1\n",
    "            end_m = end_month if year == end_year else 12\n",
    "            year_dir = f\"{remote_dir}/{year}\"\n",
    "            try:\n",
    "                ftp.cwd(year_dir)\n",
    "                print(f\"Entering directory: {year_dir}\")\n",
    "            except ftplib.error_perm:\n",
    "                print(f\"Skipping {year_dir}, not a directory.\")\n",
    "                continue\n",
    "\n",
    "            # List all month directories\n",
    "            months = ftp.nlst()\n",
    "            months.sort()\n",
    "            for month in months:\n",
    "                if int(month) < start_m or int(month) > end_m:\n",
    "                    continue\n",
    "                month_dir = f\"{year_dir}/{month}\"\n",
    "\n",
    "                try:\n",
    "                    ftp.cwd(month_dir)\n",
    "                    print(f\"Entering directory: {month_dir}\")\n",
    "                except ftplib.error_perm:\n",
    "                    print(f\"Skipping {month_dir}, not a directory.\")\n",
    "                    continue\n",
    "\n",
    "                # List all day directories\n",
    "                days = ftp.nlst()\n",
    "                days.sort()\n",
    "                for day in days:\n",
    "                    day_dir = f\"{month_dir}/{day}\"\n",
    "                    try:\n",
    "                        ftp.cwd(day_dir)\n",
    "                        print(f\"Entering directory: {day_dir}\")\n",
    "                    except ftplib.error_perm:\n",
    "                        print(f\"Skipping {day_dir}, not a directory.\")\n",
    "                        continue\n",
    "\n",
    "                    # List all .gz files\n",
    "                    files = ftp.nlst()\n",
    "                    gz_files = [file for file in files if file.endswith('.gz')]\n",
    "\n",
    "                    # Ensure local day directory exists\n",
    "                    local_day_dir = os.path.join(local_dir, str(year), month, day)\n",
    "                    if not os.path.exists(local_day_dir):\n",
    "                        os.makedirs(local_day_dir)\n",
    "\n",
    "                    # Download and process each .gz file\n",
    "                    for file in gz_files:\n",
    "                        print(f\"Processing file: {file}\")\n",
    "                        download_and_process_file(ftp, file, local_day_dir, extracted_dir, area_of_interest, year, month)\n",
    "\n",
    "                    # Move back to the month directory\n",
    "                    ftp.cwd('..')\n",
    "\n",
    "                # Move back to the year directory\n",
    "                ftp.cwd('..')\n",
    "\n",
    "            # Move back to the root directory\n",
    "            ftp.cwd('..')\n",
    "\n",
    "        # Close the FTP connection\n",
    "        ftp.quit()\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ftp_host = 'hokusai.eorc.jaxa.jp'\n",
    "    ftp_user = 'rainmap'\n",
    "    ftp_password = 'Niskur+1404'\n",
    "    remote_dir = '/realtime_ver/v8/archive'\n",
    "    local_dir = r'c:\\Users\\Jun Wei\\Desktop\\GSMaP_files\\temp'\n",
    "    extracted_dir = r'c:\\Users\\Jun Wei\\Desktop\\GSMaP_files\\extracted'\n",
    "    \n",
    "    start_year = 2023  # <---- Change dates here\n",
    "    start_month = 12\n",
    "    end_year = 2023 \n",
    "    end_month = 12\n",
    "    # CHANGE AREA HERE\n",
    "    area_of_interest = {'lat_min': -7.29, 'lon_min': 93.16, 'lat_max': 9.972, 'lon_max': 110.422}\n",
    "    \n",
    "    # Download and process files\n",
    "    download_gz_files(ftp_host, ftp_user, ftp_password, remote_dir, local_dir, extracted_dir, start_year, end_year, area_of_interest)\n",
    "\n",
    "    print(\"Download, extraction, and conversion processes completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge and compress\n",
    "\n",
    "This code block will merge the extracted files by month and compress the combined file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import netCDF4 as nc\n",
    "from dask.diagnostics import ProgressBar\n",
    "import pandas as pd\n",
    "folder_path = r\"c:\\Users\\Jun Wei\\Desktop\\GSMaP_files\\extracted\"\n",
    "def merge(month):\n",
    "    month_path = os.path.join(folder_path, year, month)\n",
    "    nc_files = [os.path.join(month_path, f) for f in os.listdir(month_path) if f.endswith('.nc')]\n",
    "    nc_files.sort()\n",
    "    \n",
    "    datasets = []\n",
    "    for file in nc_files:\n",
    "        # Extract datetime from filename\n",
    "        filename = os.path.basename(file)\n",
    "        date_str = filename.split('.')[1]\n",
    "        time_str = filename.split('.')[2]\n",
    "        dt = pd.to_datetime(f\"{date_str} {time_str}\", format='%Y%m%d %H%M')\n",
    "        \n",
    "        # Open dataset and assign time dimension\n",
    "        ds = xr.open_dataset(file)\n",
    "        ds = ds.expand_dims({'time': [dt]})\n",
    "        datasets.append(ds)\n",
    "    \n",
    "    try:\n",
    "        combined = xr.concat(datasets, dim='time')\n",
    "        output_path = os.path.join(folder_path, year,month, f\"{int(str(year+month))}_combined.nc\")\n",
    "        \n",
    "        encoding = {var: {\"zlib\": True, \"complevel\": 9} for var in combined.data_vars}\n",
    "        \n",
    "        with ProgressBar():\n",
    "            combined.to_netcdf(output_path, encoding=encoding)\n",
    "        \n",
    "        compress_netcdf(output_path)\n",
    "        print(f\"{month} combined and compressed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"{month} failed: {e}\")\n",
    "\n",
    "def compress_netcdf(file):\n",
    "    src_dataset = nc.Dataset(file, 'r')\n",
    "    compressed_file = file.replace(\".nc\", \"_compressed.nc\")\n",
    "    dst_dataset = nc.Dataset(compressed_file, 'w', format='NETCDF4')\n",
    "    \n",
    "    for attr in src_dataset.ncattrs():\n",
    "        dst_dataset.setncattr(attr, src_dataset.getncattr(attr))\n",
    "    for dim_name, dim in src_dataset.dimensions.items():\n",
    "        dst_dataset.createDimension(dim_name, len(dim) if not dim.isunlimited() else None)\n",
    "    for var_name, var in src_dataset.variables.items():\n",
    "        dst_var = dst_dataset.createVariable(var_name, var.datatype, var.dimensions, zlib=True, complevel=9, shuffle=True)\n",
    "        dst_var.setncatts({k: var.getncattr(k) for k in var.ncattrs()})\n",
    "        dst_var[:] = var[:]\n",
    "    \n",
    "    dst_dataset.close()\n",
    "    src_dataset.close()\n",
    "    print(f\"{file} compressed\")\n",
    "\n",
    "folder_path = r\"c:\\Users\\Jun Wei\\Desktop\\GSMaP_files\\extracted\"\n",
    "os.listdir(folder_path)\n",
    "for year in os.listdir(folder_path):\n",
    "    #if year == \"2023\":\n",
    "    for month in os.listdir(os.path.join(folder_path, year)):\n",
    "        merge(month)\n",
    "        print(f\"{year} {month} done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use this for 5 minute intervals matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import PatternFill\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Function to convert timestamp to date and time\n",
    "def split_timestamp(timestamp):\n",
    "    date, time_with_offset = timestamp.split('T')\n",
    "    time = time_with_offset.split('+')[0]  # Removing the timezone offset\n",
    "    return date, time\n",
    "\n",
    "# Function to fetch rainfall data for a specific date\n",
    "def fetch_rainfall_data(date):\n",
    "    url = \"https://api.data.gov.sg/v1/environment/rainfall\"\n",
    "    params = {'date': date}\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        return response.json(), date\n",
    "    else:\n",
    "        print(f\"Failed to fetch data for {date}, status code: {response.status_code}\")\n",
    "        return None, date\n",
    "\n",
    "# Function to process data and save to Excel with conditional formatting\n",
    "def process_and_save_data(result):\n",
    "    data, date = result\n",
    "    if data is None:\n",
    "        print(f\"No data to process for {date}\")\n",
    "        return\n",
    "\n",
    "    stations = {station['id']: station for station in data['metadata']['stations']}\n",
    "    rain_data = {}\n",
    "\n",
    "    for item in data['items']:\n",
    "        timestamp = item['timestamp']\n",
    "        date, time = split_timestamp(timestamp)\n",
    "        key = (date, time)\n",
    "        \n",
    "        if key not in rain_data:\n",
    "            rain_data[key] = {}\n",
    "            \n",
    "        readings = item['readings']\n",
    "        for reading in readings:\n",
    "            station_id = reading['station_id']\n",
    "            if station_id in stations:\n",
    "                station_info = stations[station_id]\n",
    "                rain_data[key][station_info['name']] = reading['value']\n",
    "            else:\n",
    "                print(f\"Station ID {station_id} not found in metadata\")\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(rain_data).T  # Transpose to get dates and times as rows\n",
    "    df.index = pd.MultiIndex.from_tuples(df.index, names=['Date', 'Time'])\n",
    "\n",
    "    # Reorder columns to have station names in the correct order\n",
    "    station_names = [stations[station_id]['name'] for station_id in stations]\n",
    "    df = df.reindex(columns=station_names)\n",
    "\n",
    "    # Reset index to have Date and Time as columns\n",
    "    df = df.reset_index()\n",
    "\n",
    "    # Create folder if it doesn't exist\n",
    "    folder_name = 'API_2023' #<-----------------change the folder name here\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    # Save to Excel\n",
    "    excel_filename = os.path.join(folder_name, f'{date}.xlsx')\n",
    "    df.to_excel(excel_filename, index=False)\n",
    "\n",
    "    # Load the workbook and the sheet\n",
    "    wb = load_workbook(excel_filename)\n",
    "    ws = wb.active\n",
    "\n",
    "    # Define the fill for non-zero values\n",
    "    highlight_fill = PatternFill(start_color=\"FFFF00\", end_color=\"FFFF00\", fill_type=\"solid\")\n",
    "\n",
    "    # Apply conditional formatting\n",
    "    for row in ws.iter_rows(min_row=2, min_col=3, max_row=ws.max_row, max_col=ws.max_column):\n",
    "        for cell in row:\n",
    "            if cell.value != 0 and cell.value is not None:\n",
    "                cell.fill = highlight_fill\n",
    "\n",
    "    # Save the workbook\n",
    "    wb.save(excel_filename)\n",
    "    print(f\"Data for {date} has been saved to '{excel_filename}'\")\n",
    "\n",
    "# Main script to iterate through dates and process data\n",
    "start_date = datetime.strptime('2023-01-01', '%Y-%m-%d') #<-----------------change the date here\n",
    "end_date = datetime.strptime('2023-12-31', '%Y-%m-%d')  #<-----------------change the date here\n",
    "\n",
    "dates = [start_date + timedelta(days=x) for x in range((end_date - start_date).days + 1)]\n",
    "\n",
    "# Using ThreadPoolExecutor to parallelize the fetching and processing of data\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    future_to_date = {executor.submit(fetch_rainfall_data, date.strftime('%Y-%m-%d')): date for date in dates}\n",
    "    \n",
    "    for future in as_completed(future_to_date):\n",
    "        try:\n",
    "            result = future.result()\n",
    "            process_and_save_data(result)\n",
    "        except Exception as e:\n",
    "            date = future_to_date[future]\n",
    "            print(f\"Error processing data for {date.strftime('%Y-%m-%d')}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run this to combine all the files from API into 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concate without summing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def read_and_format(file_path):\n",
    "    \"\"\"Read an Excel file and return a dictionary of DataFrames with combined 'Date' and 'Time' index or 'DateTime' index.\"\"\"\n",
    "    xls = pd.ExcelFile(file_path)\n",
    "    sheet_dfs = {}\n",
    "    for sheet_name in xls.sheet_names:\n",
    "        df = pd.read_excel(xls, sheet_name=sheet_name)\n",
    "        if 'Date' in df.columns and 'Time' in df.columns:\n",
    "            df['DateTime'] = pd.to_datetime(df['Date'].astype(str) + ' ' + df['Time'].astype(str))\n",
    "            df = df.drop(columns=['Date', 'Time'])\n",
    "        elif 'DateTime' in df.columns:\n",
    "            df['DateTime'] = pd.to_datetime(df['DateTime'])\n",
    "        else:\n",
    "            print(f\"Sheet {sheet_name} in file {file_path} does not have the expected date columns.\")\n",
    "            continue\n",
    "        df = df.set_index('DateTime')\n",
    "        sheet_dfs[sheet_name] = df\n",
    "    return sheet_dfs\n",
    "\n",
    "def process_files(folder_path):\n",
    "    \"\"\"Process each file in the folder and concatenate the data from all sheets.\"\"\"\n",
    "    combined_data = {}\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.xlsx'):  # Adjust the extension if your files are Excel files\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            try:\n",
    "                sheet_dfs = read_and_format(file_path)\n",
    "                for sheet_name, df in sheet_dfs.items():\n",
    "                    if sheet_name not in combined_data:\n",
    "                        combined_data[sheet_name] = []\n",
    "                    combined_data[sheet_name].append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "    # Combine data from each sheet separately\n",
    "    for sheet_name in combined_data:\n",
    "        combined_data[sheet_name] = pd.concat(combined_data[sheet_name])\n",
    "        combined_data[sheet_name] = combined_data[sheet_name].sort_index()  # Sort by datetime index\n",
    "\n",
    "    return combined_data\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = r'C:\\Users\\userAdmin\\Desktop\\Codes\\Data Extraction\\API_2023'  # Update with the path to your folder\n",
    "\n",
    "# Process the files and get the result\n",
    "combined_data = process_files(folder_path)\n",
    "\n",
    "# Save the result to a new Excel file with multiple sheets\n",
    "output_file_path = '2023_spring_annual.xlsx'\n",
    "with pd.ExcelWriter(output_file_path) as writer:\n",
    "    for sheet_name, df in combined_data.items():\n",
    "        df.to_excel(writer, sheet_name=sheet_name)\n",
    "\n",
    "print(f\"Processed all files and saved the result to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relative humidity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Function to convert timestamp to date and time\n",
    "def split_timestamp(timestamp):\n",
    "    date, time_with_offset = timestamp.split('T')\n",
    "    time = time_with_offset.split('+')[0]  # Removing the timezone offset\n",
    "    return date, time\n",
    "\n",
    "# Function to fetch relative humidity data for a specific date\n",
    "def fetch_relative_humidity_data(date):\n",
    "    url = \"https://api.data.gov.sg/v1/environment/relative-humidity\"\n",
    "    params = {'date': date}\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        return response.json(), date\n",
    "    else:\n",
    "        print(f\"Failed to fetch data for {date}, status code: {response.status_code}\")\n",
    "        return None, date\n",
    "\n",
    "# Function to process data and save to Excel with interpolation for missing data\n",
    "def process_and_save_data(result):\n",
    "    data, date = result\n",
    "    if data is None:\n",
    "        print(f\"No data to process for {date}\")\n",
    "        return\n",
    "\n",
    "    stations = {station['id']: station for station in data['metadata']['stations']}\n",
    "    humidity_data = {}\n",
    "\n",
    "    for item in data['items']:\n",
    "        timestamp = item['timestamp']\n",
    "        date, time = split_timestamp(timestamp)\n",
    "        key = (date, time)\n",
    "        \n",
    "        if key not in humidity_data:\n",
    "            humidity_data[key] = {}\n",
    "            \n",
    "        readings = item['readings']\n",
    "        for reading in readings:\n",
    "            station_id = reading['station_id']\n",
    "            if station_id in stations:\n",
    "                station_info = stations[station_id]\n",
    "                humidity_data[key][station_info['name']] = reading['value']\n",
    "            else:\n",
    "                print(f\"Station ID {station_id} not found in metadata\")\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(humidity_data).T  # Transpose to get dates and times as rows\n",
    "    df.index = pd.MultiIndex.from_tuples(df.index, names=['Date', 'Time'])\n",
    "\n",
    "    # Reorder columns to have station names in the correct order\n",
    "    station_names = [stations[station_id]['name'] for station_id in stations]\n",
    "    df = df.reindex(columns=station_names)\n",
    "\n",
    "    # Reset index to have Date and Time as columns\n",
    "    df = df.reset_index()\n",
    "\n",
    "    # Convert object columns to numeric\n",
    "    df[station_names] = df[station_names].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Interpolate missing data within each column and round to 1 decimal place\n",
    "    df.interpolate(method='linear', axis=0, inplace=True)\n",
    "    df = df.round(1)\n",
    "\n",
    "    # Create folder if it doesn't exist\n",
    "    folder_name = 'API_2023_Spring_Relative_Humidity' #<-----------------change the folder name here\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    # Create a new workbook\n",
    "    excel_filename = os.path.join(folder_name, f'{date}_relative_humidity.xlsx')\n",
    "    wb = Workbook()\n",
    "\n",
    "    # Add original data to the first sheet\n",
    "    ws1 = wb.active\n",
    "    ws1.title = \"Original Data\"\n",
    "    for r in dataframe_to_rows(df, index=False, header=True):\n",
    "        ws1.append(r)\n",
    "\n",
    "    # Group data into 5-minute intervals and add to a new sheet\n",
    "    df['DateTime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])\n",
    "    df.set_index('DateTime', inplace=True)\n",
    "\n",
    "    # Create an empty DataFrame to store 5-minute grouped data\n",
    "    df_5min = pd.DataFrame()\n",
    "\n",
    "    for station in station_names:\n",
    "        station_data = df[[station]].resample('5T').mean()\n",
    "        df_5min = pd.concat([df_5min, station_data], axis=1)\n",
    "\n",
    "    df_5min.reset_index(inplace=True)\n",
    "    df_5min = df_5min.round(1)\n",
    "    # Create a new sheet for 5-minute data\n",
    "    ws2 = wb.create_sheet(title=\"5-Minute Data\")\n",
    "    for r in dataframe_to_rows(df_5min, index=False, header=True):\n",
    "        ws2.append(r)\n",
    "\n",
    "    # Save the workbook\n",
    "    wb.save(excel_filename)\n",
    "    print(f\"Data for {date} has been saved to '{excel_filename}'\")\n",
    "\n",
    "# Main script to iterate through dates and process data\n",
    "start_date = datetime.strptime('2023-01-01', '%Y-%m-%d') #<-----------------change the date here\n",
    "end_date = datetime.strptime('2023-03-31', '%Y-%m-%d')  #<-----------------change the date here\n",
    "\n",
    "dates = [start_date + timedelta(days=x) for x in range((end_date - start_date).days + 1)]\n",
    "\n",
    "# Using ThreadPoolExecutor to parallelize the fetching and processing of data\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    future_to_date = {executor.submit(fetch_relative_humidity_data, date.strftime('%Y-%m-%d')): date for date in dates}\n",
    "    \n",
    "    for future in as_completed(future_to_date):\n",
    "        try:\n",
    "            result = future.result()\n",
    "            process_and_save_data(result)\n",
    "        except Exception as e:\n",
    "            date = future_to_date[future]\n",
    "            print(f\"Error processing data for {date.strftime('%Y-%m-%d')}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Function to convert timestamp to date and time\n",
    "def split_timestamp(timestamp):\n",
    "    date, time_with_offset = timestamp.split('T')\n",
    "    time = time_with_offset.split('+')[0]  # Removing the timezone offset\n",
    "    return date, time\n",
    "\n",
    "# Function to fetch temperature data for a specific date\n",
    "def fetch_temperature_data(date):\n",
    "    url = \"https://api.data.gov.sg/v1/environment/air-temperature\"\n",
    "    params = {'date': date}\n",
    "    response = requests.get(url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        return response.json(), date\n",
    "    else:\n",
    "        print(f\"Failed to fetch data for {date}, status code: {response.status_code}\")\n",
    "        return None, date\n",
    "\n",
    "# Function to process data and save to Excel with interpolation for missing data\n",
    "def process_and_save_data(result):\n",
    "    data, date = result\n",
    "    if data is None:\n",
    "        print(f\"No data to process for {date}\")\n",
    "        return\n",
    "\n",
    "    stations = {station['id']: station for station in data['metadata']['stations']}\n",
    "    temperature_data = {}\n",
    "\n",
    "    for item in data['items']:\n",
    "        timestamp = item['timestamp']\n",
    "        date, time = split_timestamp(timestamp)\n",
    "        key = (date, time)\n",
    "        \n",
    "        if key not in temperature_data:\n",
    "            temperature_data[key] = {}\n",
    "            \n",
    "        readings = item['readings']\n",
    "        for reading in readings:\n",
    "            station_id = reading['station_id']\n",
    "            if station_id in stations:\n",
    "                station_info = stations[station_id]\n",
    "                temperature_data[key][station_info['name']] = reading['value']\n",
    "            else:\n",
    "                print(f\"Station ID {station_id} not found in metadata\")\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(temperature_data).T  # Transpose to get dates and times as rows\n",
    "    df.index = pd.MultiIndex.from_tuples(df.index, names=['Date', 'Time'])\n",
    "\n",
    "    # Reorder columns to have station names in the correct order\n",
    "    station_names = [stations[station_id]['name'] for station_id in stations]\n",
    "    df = df.reindex(columns=station_names)\n",
    "\n",
    "    # Reset index to have Date and Time as columns\n",
    "    df = df.reset_index()\n",
    "\n",
    "    # Convert object columns to numeric\n",
    "    df[station_names] = df[station_names].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Interpolate missing data within each column and round to 1 decimal place\n",
    "    df.interpolate(method='linear', axis=0, inplace=True)\n",
    "    df = df.round(1)\n",
    "\n",
    "    # Create folder if it doesn't exist\n",
    "    folder_name = 'API_2023_Spring_Temperature' #<-----------------change the folder name here\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    # Create a new workbook\n",
    "    excel_filename = os.path.join(folder_name, f'{date}_temperature.xlsx')\n",
    "    wb = Workbook()\n",
    "\n",
    "    # Add original data to the first sheet\n",
    "    ws1 = wb.active\n",
    "    ws1.title = \"Original Data\"\n",
    "    for r in dataframe_to_rows(df, index=False, header=True):\n",
    "        ws1.append(r)\n",
    "\n",
    "    # Group data into 5-minute intervals and add to a new sheet\n",
    "    df['DateTime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])\n",
    "    df.set_index('DateTime', inplace=True)\n",
    "\n",
    "    # Create an empty DataFrame to store 5-minute grouped data\n",
    "    df_5min = pd.DataFrame()\n",
    "\n",
    "    for station in station_names:\n",
    "        station_data = df[[station]].resample('5T').mean()\n",
    "        df_5min = pd.concat([df_5min, station_data], axis=1)\n",
    "\n",
    "    df_5min.reset_index(inplace=True)\n",
    "    df_5min = df_5min.round(1)  # Round 5-minute grouped data to 1 decimal place\n",
    "\n",
    "    # Create a new sheet for 5-minute data\n",
    "    ws2 = wb.create_sheet(title=\"5-Minute Data\")\n",
    "    for r in dataframe_to_rows(df_5min, index=False, header=True):\n",
    "        ws2.append(r)\n",
    "\n",
    "    # Save the workbook\n",
    "    wb.save(excel_filename)\n",
    "    print(f\"Data for {date} has been saved to '{excel_filename}'\")\n",
    "\n",
    "# Main script to iterate through dates and process data\n",
    "start_date = datetime.strptime('2023-01-01', '%Y-%m-%d') #<-----------------change the date here\n",
    "end_date = datetime.strptime('2023-03-31', '%Y-%m-%d')  #<-----------------change the date here\n",
    "\n",
    "dates = [start_date + timedelta(days=x) for x in range((end_date - start_date).days + 1)]\n",
    "\n",
    "# Using ThreadPoolExecutor to parallelize the fetching and processing of data\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    future_to_date = {executor.submit(fetch_temperature_data, date.strftime('%Y-%m-%d')): date for date in dates}\n",
    "    \n",
    "    for future in as_completed(future_to_date):\n",
    "        try:\n",
    "            result = future.result()\n",
    "            process_and_save_data(result)\n",
    "        except Exception as e:\n",
    "            date = future_to_date[future]\n",
    "            print(f\"Error processing data for {date.strftime('%Y-%m-%d')}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

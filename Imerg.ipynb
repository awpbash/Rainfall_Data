{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download from links list (IMERG FINAL) \n",
    "\n",
    "remove first 2 line in text file\n",
    "\n",
    "Use python 3.11, DO NOT USE 3.12\n",
    "\n",
    "Just run the first script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "import xarray as xr\n",
    "from dask.diagnostics import ProgressBar\n",
    "import netCDF4 as nc\n",
    "\n",
    "# Define the base download directory\n",
    "base_download_path = r\"C:\\Users\\Jun Wei\\Desktop\\IMERG data files\"\n",
    "os.makedirs(base_download_path, exist_ok=True)\n",
    "\n",
    "# Read the URLs from the text file\n",
    "urls_file = r\"c:\\Users\\Jun Wei\\Downloads\\subset_GPM_3IMERGHH_07_20240701_005823_.txt\" #<--- Change this to the path of the text file containing the URLs\n",
    "try:\n",
    "    df = pd.read_csv(urls_file, header=None, delimiter=\"\\t\", on_bad_lines='warn')\n",
    "    urls = df[0][:].tolist()\n",
    "except Exception as e:\n",
    "    print(f\"Error reading URLs file: {e}\")\n",
    "    urls = []\n",
    "\n",
    "# Function to get filenames and month directories from URLs\n",
    "def get_filename_and_month_from_url(url):\n",
    "    date_time_index = url.find(\"IMERG.\")\n",
    "    if date_time_index != -1:\n",
    "        date_time_part = url[date_time_index+6:date_time_index+15] + url[date_time_index+16:date_time_index+20]\n",
    "        filename = date_time_part + \".nc\"\n",
    "        month_dir = date_time_part[:6]\n",
    "        return filename, month_dir\n",
    "    else:\n",
    "        return \"unknown_date_time.nc\", \"unknown\"\n",
    "\n",
    "# Create a mapping of URLs to filenames and month directories\n",
    "url_to_filename_and_month = {url: get_filename_and_month_from_url(url) for url in urls}\n",
    "\n",
    "# Function to download a file\n",
    "def download_file(session, url, filename, month_dir):\n",
    "    month_path = os.path.join(base_download_path, month_dir)\n",
    "    os.makedirs(month_path, exist_ok=True)\n",
    "    filepath = os.path.join(month_path, filename)\n",
    "    attempts = 0\n",
    "    max_attempts = 3\n",
    "    backoff_time = 1.5  # start with 1 second\n",
    "\n",
    "    while attempts < max_attempts:\n",
    "        try:\n",
    "            res = session.get(url)\n",
    "            res.raise_for_status()\n",
    "            with open(filepath, 'wb') as f:\n",
    "                f.write(res.content)\n",
    "            return f\"{filename} downloaded successfully to {month_dir}.\"\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            attempts += 1\n",
    "            if attempts < max_attempts:\n",
    "                time.sleep(backoff_time)\n",
    "                backoff_time *= 1  # exponential backoff\n",
    "            else:\n",
    "                return f\"Failed to download {filename} after {attempts} attempts: {e}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error saving {filename}: {e}\"\n",
    "\n",
    "# Set up the session with retries\n",
    "session = requests.Session()\n",
    "retry = Retry(total=5, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])\n",
    "adapter = HTTPAdapter(max_retries=retry)\n",
    "session.mount('http://', adapter)\n",
    "session.mount('https://', adapter)\n",
    "\n",
    "# Download the files for a specific month\n",
    "def download_files_for_month(month_dir, url_to_filename_and_month):\n",
    "    month_urls = {url: (filename, md) for url, (filename, md) in url_to_filename_and_month.items() if md == month_dir}\n",
    "    with ThreadPoolExecutor(max_workers=50) as executor:\n",
    "        future_to_url = {executor.submit(download_file, session, url, filename, month_dir): url for url, (filename, _) in month_urls.items()}\n",
    "        for future in as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                print(result)\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading {url}: {e}\")\n",
    "\n",
    "# Check for missing files and redownload if necessary for a specific month\n",
    "def check_and_redownload_for_month(month_dir, url_to_filename_and_month):\n",
    "    month_path = os.path.join(base_download_path, month_dir)\n",
    "    actual_files = os.listdir(month_path) if os.path.exists(month_path) else []\n",
    "    \n",
    "    # Identify missing files\n",
    "    missing_files = [filename for url, (filename, md) in url_to_filename_and_month.items() if md == month_dir and filename not in actual_files]\n",
    "    \n",
    "    # If there are missing files, reconstruct their URLs and redownload them\n",
    "    if missing_files:\n",
    "        print(f\"Retrying missing files for {month_dir}...\")\n",
    "        missing_url_to_filename_and_month = {url: (filename, md) for url, (filename, md) in url_to_filename_and_month.items() if filename in missing_files}\n",
    "        download_files_for_month(month_dir, missing_url_to_filename_and_month)\n",
    "    \n",
    "    # Final check to ensure all files are downloaded\n",
    "    actual_files = os.listdir(month_path) if os.path.exists(month_path) else []\n",
    "    still_missing = [filename for url, (filename, md) in url_to_filename_and_month.items() if md == month_dir and filename not in actual_files]\n",
    "    \n",
    "    if still_missing:\n",
    "        print(f\"Some files are still missing in {month_dir}:\")\n",
    "        for filename in still_missing:\n",
    "            print(filename)\n",
    "    else:\n",
    "        print(f\"All files downloaded successfully for {month_dir}.\")\n",
    "\n",
    "# Combine files for a specific month\n",
    "def combine_files_for_month(month_dir):\n",
    "    month_path = os.path.join(base_download_path, month_dir)\n",
    "    \n",
    "    # List all .nc files in the month directory\n",
    "    nc_files = [os.path.join(month_path, f) for f in os.listdir(month_path) if f.endswith('.nc')]\n",
    "    \n",
    "    # Sort the files to ensure they are in chronological order\n",
    "    nc_files.sort()\n",
    "    \n",
    "    # Define the total number of files for progress tracking\n",
    "    total_files = len(nc_files)\n",
    "    print(f\"Total number of files to combine for {month_dir}: {total_files}\")\n",
    "\n",
    "    try:\n",
    "        # Open multiple files as a single xarray Dataset using dask\n",
    "        combined_dataset = xr.open_mfdataset(nc_files, combine='by_coords', parallel=True, engine='netcdf4')\n",
    "\n",
    "        # Define the output file path\n",
    "        output_file = os.path.join(month_path, f\"{month_dir}.nc\")\n",
    "\n",
    "        # Specify encoding to avoid OverflowError\n",
    "        encoding = {}\n",
    "        for var in combined_dataset.variables:\n",
    "            if var == 'time':\n",
    "                encoding[var] = {'dtype': 'int64'}\n",
    "            else:\n",
    "                if combined_dataset[var].dtype == 'int64':\n",
    "                    encoding[var] = {'dtype': 'int32'}\n",
    "                else:\n",
    "                    encoding[var] = {}\n",
    "\n",
    "        # Save the combined dataset to a new NetCDF file with progress tracking\n",
    "        with ProgressBar():\n",
    "            combined_dataset.to_netcdf(output_file, encoding=encoding, compute=True)\n",
    "\n",
    "        print(f\"Combined NetCDF file saved to {output_file}\")\n",
    "\n",
    "        # Compress the combined NetCDF file\n",
    "        compress_netcdf(output_file)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error combining NetCDF files for {month_dir}: {e}\")\n",
    "\n",
    "# Compress the NetCDF file\n",
    "def compress_netcdf(file_path):\n",
    "    # Open the source file (uncompressed)\n",
    "    src_dataset = nc.Dataset(file_path, 'r')\n",
    "\n",
    "    # Create a new compressed file\n",
    "    compressed_file_path = file_path.replace('.nc', '_compressed.nc')\n",
    "    dst_dataset = nc.Dataset(compressed_file_path, 'w', format='NETCDF4')\n",
    "\n",
    "    # Copy global attributes\n",
    "    for attr_name in src_dataset.ncattrs():\n",
    "        dst_dataset.setncattr(attr_name, src_dataset.getncattr(attr_name))\n",
    "\n",
    "    # Copy dimensions\n",
    "    for dim_name, dim in src_dataset.dimensions.items():\n",
    "        dst_dataset.createDimension(dim_name, len(dim) if not dim.isunlimited() else None)\n",
    "\n",
    "    # Copy variables with enhanced compression settings\n",
    "    for var_name, var in src_dataset.variables.items():\n",
    "        # Create variable with compression settings\n",
    "        dst_var = dst_dataset.createVariable(\n",
    "            var_name, var.datatype, var.dimensions, \n",
    "            zlib=True, complevel=9, shuffle=True  # Set compression level to maximum (9) and enable shuffle\n",
    "        )\n",
    "        # Copy variable attributes\n",
    "        dst_var.setncatts({attr: var.getncattr(attr) for attr in var.ncattrs()})\n",
    "        # Copy variable data\n",
    "        dst_var[:] = var[:]\n",
    "\n",
    "    # Close the datasets\n",
    "    src_dataset.close()\n",
    "    dst_dataset.close()\n",
    "\n",
    "    print(f'Compressed file created: {compressed_file_path}')\n",
    "\n",
    "# Process each month\n",
    "def process_months(url_to_filename_and_month):\n",
    "    for month_dir in sorted(set(md for _, md in url_to_filename_and_month.values())):\n",
    "        print(f\"Processing month: {month_dir}\")\n",
    "        \n",
    "        # Download files for the month\n",
    "        download_files_for_month(month_dir, url_to_filename_and_month)\n",
    "        \n",
    "        # Check and redownload missing files\n",
    "        check_and_redownload_for_month(month_dir, url_to_filename_and_month)\n",
    "\n",
    "        # Combine files for the month\n",
    "        combine_files_for_month(month_dir)\n",
    "\n",
    "process_months(url_to_filename_and_month)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Everything below here is just random code for debugging\n",
    "\n",
    "This first block was used to download files then check which files are missing before downloading the missing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "# Define the download directory\n",
    "download_path = r\"C:\\Users\\Jun Wei\\Desktop\\IMERG data files\\2020-01-01\\data files\"\n",
    "os.makedirs(download_path, exist_ok=True)\n",
    "\n",
    "# Read the URLs from the text file\n",
    "urls_file = r\"c:\\Users\\Jun Wei\\Downloads\\subset_GPM_3IMERGHH_07_20240530_024229_.txt\"\n",
    "try:\n",
    "    df = pd.read_csv(urls_file, header=None, delimiter=\"\\t\", on_bad_lines='warn')\n",
    "    urls = df[0].tolist()\n",
    "except Exception as e:\n",
    "    print(f\"Error reading URLs file: {e}\")\n",
    "    urls = []\n",
    "\n",
    "# Function to get filenames from URLs\n",
    "def get_filename_from_url(url):\n",
    "    date_time_index = url.find(\"IMERG.\")\n",
    "    if date_time_index != -1:\n",
    "        date_time_part = url[date_time_index+6:date_time_index+15] + url[date_time_index+16:date_time_index+20]\n",
    "        return date_time_part + \".nc\"\n",
    "    else:\n",
    "        return \"unknown_date_time.nc\"\n",
    "\n",
    "# Create a mapping of URLs to filenames\n",
    "url_to_filename = {url: get_filename_from_url(url) for url in urls}\n",
    "\n",
    "# Function to download a file\n",
    "def download_file(session, url, filename):\n",
    "    filepath = os.path.join(download_path, filename)\n",
    "    attempts = 0\n",
    "    max_attempts = 3\n",
    "    backoff_time = 1.5  # start with 1 second\n",
    "\n",
    "    while attempts < max_attempts:\n",
    "        try:\n",
    "            res = session.get(url)\n",
    "            res.raise_for_status()\n",
    "            with open(filepath, 'wb') as f:\n",
    "                f.write(res.content)\n",
    "            return f\"{filename} downloaded successfully.\"\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            attempts += 1\n",
    "            if attempts < max_attempts:\n",
    "                time.sleep(backoff_time)\n",
    "                backoff_time *= 1  # exponential backoff\n",
    "            else:\n",
    "                return f\"Failed to download {filename} after {attempts} attempts: {e}\"\n",
    "        except Exception as e:\n",
    "            return f\"Error saving {filename}: {e}\"\n",
    "\n",
    "# Set up the session with retries\n",
    "session = requests.Session()\n",
    "retry = Retry(total=5, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])\n",
    "adapter = HTTPAdapter(max_retries=retry)\n",
    "session.mount('http://', adapter)\n",
    "session.mount('https://', adapter)\n",
    "\n",
    "# Download the files initially\n",
    "def download_files(url_to_filename):\n",
    "    with ThreadPoolExecutor(max_workers=100) as executor:\n",
    "        future_to_url = {executor.submit(download_file, session, url, filename): url for url, filename in url_to_filename.items()}\n",
    "        for future in as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                print(result)\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading {url}: {e}\")\n",
    "\n",
    "download_files(url_to_filename)\n",
    "\n",
    "# Check for missing files and redownload if necessary\n",
    "def check_and_redownload(url_to_filename):\n",
    "    # List the actual downloaded files\n",
    "    actual_files = os.listdir(download_path)\n",
    "    \n",
    "    # Identify missing files\n",
    "    missing_files = [filename for filename in url_to_filename.values() if filename not in actual_files]\n",
    "    \n",
    "    # If there are missing files, reconstruct their URLs and redownload them\n",
    "    if missing_files:\n",
    "        print(\"Retrying missing files...\")\n",
    "        missing_url_to_filename = {url: filename for url, filename in url_to_filename.items() if filename in missing_files}\n",
    "        download_files(missing_url_to_filename)\n",
    "    \n",
    "    # Final check to ensure all files are downloaded\n",
    "    actual_files = os.listdir(download_path)\n",
    "    still_missing = [filename for filename in url_to_filename.values() if filename not in actual_files]\n",
    "    \n",
    "    if still_missing:\n",
    "        print(\"Some files are still missing:\")\n",
    "        for filename in still_missing:\n",
    "            print(filename)\n",
    "    else:\n",
    "        print(\"All files downloaded successfully.\")\n",
    "\n",
    "check_and_redownload(url_to_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to compress combined NC file\n",
    "\n",
    "Saves space by chunking data and reducing overhead, however this means it might take slightly longer to run programn operations on the compressed file as the program has to recalculate everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_netcdf(file_path):\n",
    "    # Open the source file (uncompressed)\n",
    "    print(\"gayaf\")\n",
    "    print(file_path)\n",
    "    src_dataset = nc.Dataset(file_path, 'r')\n",
    "    print('gay')\n",
    "    # Create a new compressed file\n",
    "    compressed_file_path = file_path.replace('.nc', '_compressed.nc')\n",
    "    print(\"gay2\")\n",
    "    dst_dataset = nc.Dataset(compressed_file_path, 'w', format='NETCDF4')\n",
    "\n",
    "    # Copy global attributes\n",
    "    for attr_name in src_dataset.ncattrs():\n",
    "        dst_dataset.setncattr(attr_name, src_dataset.getncattr(attr_name))\n",
    "\n",
    "    # Copy dimensions\n",
    "    for dim_name, dim in src_dataset.dimensions.items():\n",
    "        dst_dataset.createDimension(dim_name, len(dim) if not dim.isunlimited() else None)\n",
    "\n",
    "    # Copy variables with enhanced compression settings\n",
    "    for var_name, var in src_dataset.variables.items():\n",
    "        # Create variable with compression settings\n",
    "        dst_var = dst_dataset.createVariable(\n",
    "            var_name, var.datatype, var.dimensions, \n",
    "            zlib=True, complevel=9, shuffle=True  # Set compression level to maximum (9) and enable shuffle\n",
    "        )\n",
    "        # Copy variable attributes\n",
    "        dst_var.setncatts({attr: var.getncattr(attr) for attr in var.ncattrs()})\n",
    "        # Copy variable data\n",
    "        dst_var[:] = var[:]\n",
    "\n",
    "    # Close the datasets\n",
    "    src_dataset.close()\n",
    "    dst_dataset.close()\n",
    "\n",
    "    print(f'Compressed file created: {compressed_file_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block does the following:\n",
    "1) Checks whether there is a compressed file in the folder, if there is it means the folder is probably complete.\n",
    "2) Checks if there are missing files in the folder, if there is, flag it in a missing list\n",
    "3) Combine the files if all files are present\n",
    "4) Compress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import dask\n",
    "from dask.diagnostics import ProgressBar\n",
    "import sys\n",
    "import netCDF4 as nc\n",
    "\n",
    "# Define the directory containing the .nc files\n",
    "data_dir = \"e:\\\\IMERG\\\\2005\\\\200510\"\n",
    "missing = []\n",
    "\n",
    "def merge_nc_files(data_dir):\n",
    "    # List all .nc files in the directory\n",
    "    nc_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.nc')]\n",
    "    year_month = data_dir[-6:]\n",
    "    compr = data_dir+\"\\\\\"+str(year_month)+\"_compressed.nc\"\n",
    "    if compr in nc_files:\n",
    "        print(f\"NetCDF files already combined for {year_month}\")\n",
    "        return\n",
    "    else:\n",
    "        if len(nc_files) % 48 != 0:\n",
    "            missing.append(year_month)\n",
    "            return\n",
    "        else:\n",
    "            # Sort the files to ensure they are in chronological order\n",
    "            nc_files.sort()\n",
    "            print(nc_files)\n",
    "            # Define the total number of files for progress tracking\n",
    "            total_files = len(nc_files)\n",
    "            print(f\"Total number of files to combine: {total_files}\")\n",
    "\n",
    "            # Open multiple files as a single xarray Dataset using dask\n",
    "\n",
    "            combined_dataset = xr.open_mfdataset(nc_files, combine='by_coords', parallel=True)\n",
    "\n",
    "            output_name = year_month + \".nc\"\n",
    "            print(output_name)\n",
    "            # Define the output file path\n",
    "            output_file = os.path.join(data_dir, output_name)\n",
    "            print(output_file)\n",
    "            # Specify encoding to avoid OverflowError\n",
    "            encoding = {}\n",
    "            for var in combined_dataset.variables:\n",
    "                if var == 'time':\n",
    "                    encoding[var] = {'dtype': 'int64'}\n",
    "                else:\n",
    "                    if combined_dataset[var].dtype == 'int64':\n",
    "                        encoding[var] = {'dtype': 'int32'}\n",
    "                    else:\n",
    "                        encoding[var] = {}\n",
    "\n",
    "            # Save the combined dataset to a new NetCDF file with progress tracking\n",
    "            with ProgressBar():\n",
    "                combined_dataset.to_netcdf(output_file, encoding=encoding, compute=True)\n",
    "\n",
    "            print(f\"Combined NetCDF file saved to {output_file}\")\n",
    "            compress_netcdf(output_file)\n",
    "\n",
    "folders = \"E:\\\\IMERG\"\n",
    "for year in os.listdir(folders):\n",
    "    year_folder = os.path.join(folders, year)\n",
    "    for month in os.listdir(year_folder):\n",
    "        month_folder = os.path.join(year_folder, month)\n",
    "        merge_nc_files(month_folder)\n",
    "print(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder_path = \"E:\\\\IMERG\"\n",
    "error_folders = []\n",
    "for year in os.listdir(folder_path):\n",
    "    year_path = os.path.join(folder_path, year)\n",
    "    for month in os.listdir(year_path):\n",
    "        month_path = os.path.join(year_path, month)\n",
    "        if len(os.listdir(month_path)) % 48 == 2:\n",
    "            if os.path.exists(os.path.join(month_path, f\"{month}.nc\")) and os.path.exists(os.path.join(month_path, f\"{month}_compressed.nc\")):\n",
    "                print(f\"{month} ok!\")\n",
    "            else:\n",
    "                error_folders.append(month)\n",
    "        else:\n",
    "            error_folders.append(month)\n",
    "print(error_folders)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import dask\n",
    "from dask.diagnostics import ProgressBar\n",
    "import sys\n",
    "import netCDF4 as nc\n",
    "def combine_files_for_month(month_dir):\n",
    "    month_path = os.path.join(base_download_path, month_dir)\n",
    "    print(month_path)\n",
    "    # List all .nc files in the month directory\n",
    "    nc_files = [os.path.join(month_path, f) for f in os.listdir(month_path) if f.endswith('.nc')]\n",
    "    \n",
    "    # Sort the files to ensure they are in chronological order\n",
    "    nc_files.sort()\n",
    "    \n",
    "    # Define the total number of files for progress tracking\n",
    "    total_files = len(nc_files)\n",
    "    print(f\"Total number of files to combine for {month_dir}: {total_files}\")\n",
    "\n",
    "    try:\n",
    "        # Open multiple files as a single xarray Dataset using dask\n",
    "        combined_dataset = xr.open_mfdataset(nc_files, combine='by_coords', parallel=True, engine='netcdf4')\n",
    "\n",
    "        # Define the output file path\n",
    "        output_file = os.path.join(month_path, f\"{month_dir}.nc\")\n",
    "\n",
    "        # Specify encoding to avoid OverflowError\n",
    "        encoding = {}\n",
    "        for var in combined_dataset.variables:\n",
    "            if var == 'time':\n",
    "                encoding[var] = {'dtype': 'int64'}\n",
    "            else:\n",
    "                if combined_dataset[var].dtype == 'int64':\n",
    "                    encoding[var] = {'dtype': 'int32'}\n",
    "                else:\n",
    "                    encoding[var] = {}\n",
    "\n",
    "        # Save the combined dataset to a new NetCDF file with progress tracking\n",
    "        with ProgressBar():\n",
    "            combined_dataset.to_netcdf(output_file, encoding=encoding, compute=True)\n",
    "\n",
    "        print(f\"Combined NetCDF file saved to {output_file}\")\n",
    "\n",
    "        # Compress the combined NetCDF file\n",
    "        compress_netcdf(output_file)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error combining NetCDF files for {month_dir}: {e}\")\n",
    "\n",
    "# Compress the NetCDF file\n",
    "def compress_netcdf(file_path):\n",
    "    # Open the source file (uncompressed)\n",
    "    print(\"gayaf\")\n",
    "    print(file_path)\n",
    "    src_dataset = nc.Dataset(file_path, 'r')\n",
    "    print('gay')\n",
    "    # Create a new compressed file\n",
    "    compressed_file_path = file_path.replace('.nc', '_compressed.nc')\n",
    "    print(\"gay2\")\n",
    "    dst_dataset = nc.Dataset(compressed_file_path, 'w', format='NETCDF4')\n",
    "\n",
    "    # Copy global attributes\n",
    "    for attr_name in src_dataset.ncattrs():\n",
    "        dst_dataset.setncattr(attr_name, src_dataset.getncattr(attr_name))\n",
    "\n",
    "    # Copy dimensions\n",
    "    for dim_name, dim in src_dataset.dimensions.items():\n",
    "        dst_dataset.createDimension(dim_name, len(dim) if not dim.isunlimited() else None)\n",
    "\n",
    "    # Copy variables with enhanced compression settings\n",
    "    for var_name, var in src_dataset.variables.items():\n",
    "        # Create variable with compression settings\n",
    "        dst_var = dst_dataset.createVariable(\n",
    "            var_name, var.datatype, var.dimensions, \n",
    "            zlib=True, complevel=9, shuffle=True  # Set compression level to maximum (9) and enable shuffle\n",
    "        )\n",
    "        # Copy variable attributes\n",
    "        dst_var.setncatts({attr: var.getncattr(attr) for attr in var.ncattrs()})\n",
    "        # Copy variable data\n",
    "        dst_var[:] = var[:]\n",
    "\n",
    "    # Close the datasets\n",
    "    src_dataset.close()\n",
    "    dst_dataset.close()\n",
    "\n",
    "    print(f'Compressed file created: {compressed_file_path}')\n",
    "# Define the directory containing the .nc files\n",
    "base_download_path = r\"e:\\IMERG\\2005\"\n",
    "for folder in os.listdir(base_download_path)[9:]:\n",
    "    print(f\"Processing {folder}\")\n",
    "    combine_files_for_month(folder)\n",
    "    print(f\"Processed {folder}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
